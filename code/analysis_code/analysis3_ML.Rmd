---
title: "analysis3_ML"
author: "Ehsan"
date: "11/5/2021"
output: html_document
---

```{r}
library(skimr)
library(dplyr) #for data processing
library(here)
library(broom) #for cleaning up output from lm()
library(rsample)
library(recipes)
library(tidymodels)
library(table1)
library(rpart)
library(rpart.plot)
library(doParallel) # for parallel computing 
library(vip)
```

#locating the data
```{r}
data_location <-here::here("data","processed_data","processeddata.rds")
data <- readRDS(data_location)
```
#have a look at the data
```{r}
glimpse(data)
```

#Part 1: Preprocessing
#Preprocessing: Feature removal. Removing coughYN, coughYN2 and myalgiaYn columns.
```{r}
data_1 <-  select(data, -c(CoughYN, WeaknessYN, CoughYN2, MyalgiaYN))
```
#checking of the number of columns actually got reduced or not
```{r}
glimpse(data_1)
```
#ordering factors for Weakness, CoughIntensity, Myalgia 
```{r}
data_2 <- mutate(data_1, Weakness = factor(Weakness, levels = c("None", "Mild","Moderate","Severe"),ordered = TRUE))
data_3 <- mutate(data_2, CoughIntensity = factor(CoughIntensity, levels = c("None", "Mild", "Moderate","Severe"),ordered = TRUE))
data_4 <- mutate(data_3, Myalgia = factor(Myalgia, levels = c("None", "Mild", "Moderate","Severe"),ordered = TRUE))
```
#checking if those three coumn are actually ordered or not.
```{r}
skim(data_4)
```
#Finding out which categories have less than 50 inputs
```{r}
table <- table1(~ . , data=data_4, overall="Total")
table
```
#dropping the two column that have <50 inputs
```{r}
ML_processed<- data_4%>%
  select(-c(Hearing, Vision))
```
#checking if it was removed successfully
```{r}
glimpse(ML_processed)
```

#Part2: Analysis
#set random seed to 123
```{r}
set.seed(123)
```
# Put 70% of the data into the training set and 30% into testing using strata=BodyTemp
```{r}
data_split <- initial_split(ML_processed, prop = 0.7, strata = BodyTemp)
```
# Create data frames for the two sets:
```{r}
train_data <- training(data_split)
test_data  <- testing(data_split)
```

# training set proportions by class
```{r}
train_data %>% 
  count(BodyTemp) %>% 
  mutate(prop = n/sum(n))
```
# test set proportions by class
```{r}
test_data %>% 
  count(BodyTemp) %>% 
  mutate(prop = n/sum(n))
```
##5-fold cross validation, 5 times repeated, stratified on `BodyTemp` for the CV folds
```{r}
folds <- vfold_cv(train_data, v = 5, repeats =5, strata= "BodyTemp")
```
#create recipe that codes categorical variables as dummy variables
```{r}
flu_recipe <- recipe(BodyTemp ~ ., data = train_data) %>%
           step_dummy(all_nominal_predictors())
```

# PArt 2: Building Null model

For a **continuous outcome**, using RMSE as our performance metric, a null-model that doesn't use any predictor information is one that always just predicts the mean of the data. We'll compute the performance of such a "model" here. It's useful for comparison with the real models. We'll print both numbers here, and then compare with our model results below. Since our performance metric is RMSE, we compute that here with the "model prediction" always just being the mean of the outcomes.

```{r}
RMSE_null_train <- sqrt(sum( (train_data$BodyTemp - mean(train_data$BodyTemp))^2 )/nrow(train_data))
RMSE_null_test <- sqrt(sum( (test_data$BodyTemp - mean(test_data$BodyTemp))^2 )/nrow(test_data))
print(RMSE_null_train)
print(RMSE_null_test)
```



#part 3: We'll use three models

#3.1: Tree model
#define the tree model [code from https://www.tidymodels.org/start/tuning/]
```{r}
tune_spec <-
  decision_tree(
    cost_complexity = tune(),
    tree_depth = tune(),
  ) %>%
  set_engine("rpart") %>%
  set_mode("regression") ##'regression' instead of 'classification'
tune_spec
```

# tuning grid specification
```{r}
tree_grid <- grid_regular(cost_complexity(),
                          tree_depth(),
                          levels = 5)
```
#define workflow for tree
```{r}
tree_workflow <- workflow() %>%
               add_model(tune_spec) %>%
               add_recipe(flu_recipe) #recipe from line 95
```

```{r, tune-tree}
#for parallel computing
#makes things faster. If not wanted, can be commented out, together with last line of this block.
ncores = 18 #adjust based on your computer. 
cl <- makePSOCKcluster(ncores)
registerDoParallel(cl)
# tuning using cross-validation and the tune_grid() function
tree_res <- 
  tree_workflow %>% 
  tune_grid(resamples = folds, grid = tree_grid, metric_set(rmse))
stopCluster(cl) #turning off parallerl cluster
```


#plotting metrics
```{r}
tree_res %>%
  autoplot()
```
#finding best model               
```{r}
best_tree <- tree_res %>%
  select_best("rmse")
best_tree
```
#finalize workflow
```{r}
final_wf <- 
  tree_workflow %>% 
  finalize_workflow(best_tree)
final_wf
```
# final workflow using the fit() function
```{r}
final_fit <- 
  final_wf %>%
  fit(train_data) 
```
#predicting outcomes for final model
```{r}
tree_pred <- predict(final_fit, train_data)
```

#Plotting final tree.

```{r}
rpart.plot(extract_fit_parsnip(final_fit)$fit)
```

Plotting observed/predicted and residuals.

```{r}
#predicted versus observed
plot(tree_pred$.pred,train_data$BodyTemp, xlim =c(97,103), ylim=c(97,103))
abline(a=0,b=1, col = 'red') #45 degree line, along which the results should fall
#residuals
plot(tree_pred$.pred-train_data$BodyTemp)
abline(a=0,b=0, col = 'red') #straight line, along which the results should fall
```

#tree performance
```{r}
tree_perfomance <- tree_res %>% show_best(n = 1)
print(tree_perfomance)
```

#Comparing the RMSE to the null model, we see that it is not much better. Based on our model evaluation, I think we can safely say here that a tree-based model is no good.
# Make two plots, model predictions from the tuned model versus actual outcomes
  
## LASSO linear model

Repeating the steps above, now for LASSO.


### LASSO setup


```{r, start-lasso}
#model
lasso_model <- linear_reg() %>%
  set_mode("regression") %>%           
  set_engine("glmnet") %>%
  set_args(penalty = tune(), mixture = 1) #mixture = 1 means we use the LASSO model
```

#workflow
```{r}
lasso_wf <- workflow() %>%
  add_model(lasso_model) %>% 
  add_recipe(flu_recipe)
```

### LASSO tuning

```{r, tune-lasso}
#parallel computing
ncores = 10 #adjust based on your computer
cl <- makePSOCKcluster(ncores)
registerDoParallel(cl)
#tuning grid
lasso_reg_grid <- tibble(penalty = 10^seq(-3, 0, length.out = 30))
#tune model
lasso_tune_res <- lasso_wf %>% 
  tune_grid(resamples = folds,
            grid = lasso_reg_grid,
            control = control_grid(save_pred = TRUE),
            metrics = metric_set(rmse)
            )
# turn off parallel cluster
stopCluster(cl)
```

### LASSO evaluation

```{r}
#see a plot of performance for different tuning parameters
lasso_tune_res %>% autoplot()
```

```{r}
# get the tuned model that performs best 
best_lasso <- lasso_tune_res %>%  select_best(metric = "rmse")
# finalize workflow with best model
best_lasso_wf <- lasso_wf %>% finalize_workflow(best_lasso)
# fitting best performing model
best_lasso_fit <- best_lasso_wf %>% 
  fit(data = train_data)
lasso_pred <- predict(best_lasso_fit, train_data)
```

Plotting LASSO variables as function of tuning parameter


```{r}
x <- best_lasso_fit$fit$fit$fit
plot(x, "lambda")
```
```{r}
tidy(extract_fit_parsnip(best_lasso_fit)) %>% filter(estimate != 0)
```

Plotting observed/predicted and residuals.

```{r}
#predicted versus observed
plot(lasso_pred$.pred,train_data$BodyTemp, xlim =c(97,103), ylim=c(97,103))
abline(a=0,b=1, col = 'red') #45 degree line, along which the results should fall
#residuals
plot(lasso_pred$.pred-train_data$BodyTemp)
abline(a=0,b=0, col = 'red') #straight line, along which the results should fall
```
#We want the points to be along the red lines in each plot. They are not.this model isn't much better either.

Looking at model performance. 

```{r}
lasso_perfomance <- lasso_tune_res %>% show_best(n = 1)
print(lasso_perfomance)
```
A somewhat lower RMSE, so a bit better performance.

## Random forest model

Repeating the steps above, now for a random forest.


### Random forest setup


```{r, start-rf}
rf_model <- rand_forest() %>%
  set_args(mtry = tune(),     
    trees = tune(),
    min_n = tune()
  ) %>%
  # select the engine/package that underlies the model
  set_engine("ranger",
             num.threads = 18, #for some reason for RF, we need to set this in the engine too
             importance = "permutation") %>%
  # choose either the continuous regression or binary classification mode
  set_mode("regression")           
```

```{r}
#workflow
rf_wf <- workflow() %>%
  add_model(rf_model) %>% 
  add_recipe(flu_recipe)
```

### Random forest tuning

```{r, tune-rf}
#parallel computing
cl <- makePSOCKcluster(ncores)
registerDoParallel(cl)
#tuning grid
rf_grid  <- expand.grid(mtry = c(3, 4, 5, 6), min_n = c(40,50,60), trees = c(500,1000)  )
# tune the model, optimizing RMSE
rf_tune_res <- rf_wf %>%
  tune_grid(
            resamples = folds, #CV object
            grid = rf_grid, # grid of values to try
            metrics = metric_set(rmse) 
  )
# turn off parallel cluster
stopCluster(cl)
```

### Random forest evaluation

```{r}
#see a plot of performance for different tuning parameters
rf_tune_res %>% autoplot()
```

```{r}
# get the tuned model that performs best 
best_rf <- rf_tune_res %>%  select_best(metric = "rmse")
# finalize workflow with best model
best_rf_wf <- rf_wf %>% finalize_workflow(best_rf)
# fitting best performing model
best_rf_fit <- best_rf_wf %>% 
  fit(data = train_data)
rf_pred <- predict(best_rf_fit, train_data)
```


For random forest models, one can't easily look at the final model. One can however look at the most important predictors for the final model.

```{r}
#pull out the fit object
x <- best_rf_fit$fit$fit$fit
#plot variable importance
vip::vip(x, num_features = 20)
```

Plotting observed/predicted and residuals.

```{r}
#predicted versus observed
plot(rf_pred$.pred,train_data$BodyTemp, xlim =c(97,103), ylim=c(97,103))
abline(a=0,b=1, col = 'red') #45 degree line, along which the results should fall
#residuals
plot(rf_pred$.pred-train_data$BodyTemp)
abline(a=0,b=0, col = 'red') #straight line, along which the results should fall
```

Looking at model performance. 

```{r}
rf_perfomance <- rf_tune_res %>% show_best(n = 1)
print(rf_perfomance)
```
#none of these models are actually any good.I'll go with the simpler LASSO. So let's give that model a final check.

# Final Model Evaluation

We'll now apply the model a single time to the test data.

```{r}
cl <- makePSOCKcluster(ncores)
registerDoParallel(cl)
# fit on the training set and evaluate on test set
final_fit <- best_lasso_wf  %>% last_fit(data_split)
stopCluster(cl)
```

#performance check
```{r}
test_performance <- final_fit %>% collect_metrics()
print(test_performance)
```
#RMSE is  almost same as training dta. So probably, we have avoided the risk of overfitting.

#And just another look at the diagnostic plots for the test data.

```{r}
test_predictions <- final_fit %>% collect_predictions()
```
Plotting observed/predicted and residuals.

```{r}
#predicted versus observed
plot(test_predictions$.pred,test_data$BodyTemp, xlim =c(97,103), ylim=c(97,103))
abline(a=0,b=1, col = 'red') #45 degree line, along which the results should fall
#residuals
plot(test_predictions$.pred-test_data$BodyTemp)
abline(a=0,b=0, col = 'red') #straight line, along which the results should fall
```

#none of the models turned out to be good at prediction with our data.